{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweeting Democracy:\n",
    "### Tweets of the 2020 Democractic Nominee Hopefuls\n",
    "\n",
    "##### Hanna Born, Thomas Malejko, & Nicole Yoder\n",
    "##### ANLY 580 (Fall 2020)\n",
    "##### 8 December 2020\n",
    "_Note: This report does not include any code as to improve readability, but detailed notebooks are linked where applicable._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its introduction in 2006, Twitter has evolved into a key platform for modern politics‚Äìoffering politicians a fast and easy way to communicate their message and priorities to the public. Under the Trump administration, Twitter continued to grow even more central to politics as the President often chose his Twitter account as the preferred medium for connecting with the American people. Twitter‚Äôs popularity as a medium for political discourse means that information from political campaigns--everything from campaign events to fundraising to policy platforms--is readily available in real-time. The 2020 Democratic Presidential Primaries were no different. At one point more than two dozen candidates vied for the opportunity represent the Democratic Party in the 2020 Presidential Election. This project examined the tweets made by seven of the most prominent candidates, from the start of August 2019 until Super Tuesday (March 2020), to understand how each candidate used this growing platform to engage prospective voters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rapid expansion of social media over the past decade has challenged the efficacy of existing analytic techniques due to the enormous quantity of data generated by these services (Facebook, Twitter, Youtube, etc.) as well as changes to how people communicate in these online forums. Subsequently, much research has been conducted to understand the feasibility and techniques required to generate useable insights from this data. Four papers, in particular, look at the practicality of conducting topic modeling and authorship detection of tweets. <br><br>\n",
    "In ‚ÄúAuthor Identification on Twitter,‚Äù Antonio Castro and Brian Lindauer show that an author can be detected with 40 percent accuracy using a regularized linear regression model that relies only on publicly available information on Twitter. Interestingly, the authors comment that tweeters can evade detection by deliberately altering ‚Äútheir writing voice, or limiting the amount of text posted,‚Äù which‚Äîwhile beneficial to the political dissenter that the author is concerned about‚Äîmay complicate our attempt to identify the originating Twitter account since most politicians likely have a multitude of personnel writing their tweets‚Äîfrom public affairs staffs, to aides, to the candidate themselves. Castro and Lindauer‚Äôs work builds upon the preeminent research conducted by Arvind Narayanan, et al. in a paper entitled, ‚ÄúOn the Feasibility of Internet-Scale Author Identification.‚Äù The latter authors showed that neural networks and regularized linear regression models perform equally well for authorship identification, once the data has been normalized, and developed enhanced evaluation metrics including improved confidence estimators. Brunna de Sousa Pereira Amorim, et al. researched classification techniques that could identify tweets that contained political content and, hence, potential electoral crimes in Brazil (for example, out-of-term political advertising is illegal).  As with the previous studies mentioned above, they found that logistic regression models far outperformed neural networks in this task, correctly identifying political tweets with nearly a 90 percent accuracy. Finally, ‚ÄúAn Evaluation of Topic Modelling Techniques for Twitter‚Äù evaluated multiple techniques for topic modeling on ‚Äòshort‚Äô documents, of which Twitter is completely comprised. This paper showed that biterm topic models outperformed‚Äîas measured by coherence scores‚ÄîLatent Dirichlet Allocation (including those modified for use on short texts) and word embedded models such as word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The dataset being used for this research was collected from 28 May to 9 June 2020 and is comprised of 13,814 tweets from seven of the most prominent 2020 Democratic Presidential Nominee hopefuls, including: Joe Biden, Pete Buttigieg, Tulsi Gabbard, Amy Klobuchar, Bernie Sanders, Tom Steyer, and Elizabeth Warren. The tweets span from 2 August 2019 (approximately the beginning of the 2020 Democratic Primary Campaign) to 2 March 2020 (Super Tuesday), a seven-month period wherein each candidate tweeted at least 1000 times. This dataset was collected via Twitter‚Äôs API, reformatted, and saved in a comma separated format that is 5,646 KB in size. In addition to the full text of the tweet, the dataset also contains information about the time, retweet count, and number of times that each tweet was favorited, in addition to, information about the user‚Äôs account at the time of the tweet such as follower count and friend count. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Cleaning\n",
    "[Cleaning Data notebook](CleaningData.ipynb)\n",
    "\n",
    "Before starting any analysis, the data required some cleaning. Since this research was focused on the text of the tweets, many columns were removed (like favorite_count, friends_count, etc.). Then duplicate tweets and those in languages other than English were dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "[EDA notebook](EDA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Exploratory Data Analysis was performed to develop a better understanding of the data set as a whole and investigate potential characteristics of each candidate's use of language that could provide any insights about authorship. \n",
    "\n",
    "| Candidate | Tweet Count |\n",
    "| -- | -- |\n",
    "| Elizabeth Warren    | 2347 |\n",
    "| Joe Biden           | 2183 |\n",
    "| Amy Klobuchar       | 2017 |\n",
    "| Tom Steyer          | 1990 |\n",
    "| Bernie Sanders      | 1861 |\n",
    "| Pete Buttigieg      | 1700 |\n",
    "| Tulsi Gabbard üå∫    | 1085 |\n",
    "\n",
    "The first bar chart provides the distributions of tweet counts by candidate for the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/EDA_1.png\" alt=\"Distribution of Tweets by Candidate\" width=\"350\" style=\"float: center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tweets per candidate appears relatively comparable with the exceptions of Rep. Tulsi Gabbard, whose tweet count appears significantly lower for the time period considered. Next, each candidate's tweets were evaluated according to length (in tokens) and type-token ratio characteristics.\n",
    "\n",
    "| Candidate      | Tokens per Tweet | SD of Tweet Length (in tokens) | Type/Token Ratio |\n",
    "| -------------- | -------------- | -------------- | -------------- | \n",
    "| Amy Klobuchar    |   33.798883  | 13.083463  |  10.843   |\n",
    "| Bernie Sanders   |   33.529924  | 11.299502  |  8.4832   |\n",
    "| Elizabeth Warren |   35.564433  | 11.464681  |  8.0392   |\n",
    "| Joe Biden        |   34.941204  | 11.669440  |  8.2022   |\n",
    "| Pete Buttigieg   |   34.545226  | 12.688918  |  10.1462  |\n",
    "| Tom Steyer       |   28.849539  | 12.371526  |  11.6813  |\n",
    "| Tulsi Gabbard üå∫ |    28.365482 | 16.832461  |  15.4796  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/EDA_2.png\" alt=\"Tweet Token/Type Ratio by Candidate\" width=\"350\" style=\"float: center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/EDA_3.png\" alt=\"Distribution of Tweet Lengths by Candidate\" width=\"800\" style=\"float: center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rep. Tulsi Gabbard's tweets appear to have the most fluctuation by length and token/type ratio. Similar observations show up later during the authorship detection analysis in Part A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/EDA_4.png\" alt=\"POS Distribution by Candidate\" width=\"700\" style=\"float: center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results from POS tagging, the distribution of POS usage across candidates is highly similar - mainly due to the structure of the english language. One notable observation is that Sen. Amy Klobuchar and Rep. Tulsi Gabbard have a higher use of proper nouns ('PROPN') than the other candidates. This observation coincides with some of the results in Part B. Topic Modeling, where we learn that these two candidates, proportionally, had more tweets aimed at campaigning and securing donations than other candidates who spent more time tweeting about issues important to their respective campaign platforms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "#### Research Objectives\n",
    "Based on the exploratory data analysis, background research, and relevant domain knowledge, the following research objectives were identified. These objectives were thoroughly evaluated using various analytic techniques and machine learning algorithms to maximize prediction capability or coherence while also maximizing the amount of insight about each objective.\n",
    "   1. Authorship Detection: Predict the author (in this case the Democractic Presidential Nominee Hopeful) given only the text of a previously unseen tweet and understand the stylistic features that most heavily influence the final prediction. \n",
    "   2. Topic Modeling: Identify the main topics discussed in the candidates‚Äô tweets overall and how the candidates differed in their use of those topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A. Authorship Detection\n",
    "[Authorship Detection notebook](AuthorshipDetection.ipynb)\n",
    "\n",
    "The process of authorship detection required a thorough exploratory analysis, above and beyond the techniques used in the previous section, to understand the stylistic characteristics for each of the seven candidates contained in this dataset. Two 'traditional' approaches were used to accomplish this--the Mendenhall Curve and Kilgariff's Chi-Squared Methods. Combining the results of those analyses with known features of stylistic significance (based on more modern approaches to this topic), new features were generated from the tweet text. These features were fed into a machine learning pipeline, which tuned the hyperparameters for and evaluated the efficacy of four distinct models. Once the optimal model was identified, information about the model's selection criteria was extracted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B. Topic Modeling\n",
    "[Topic Modeling notebook](TopicModeling.ipynb)\n",
    "\n",
    "Two methods of topic modeling, Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF), were used and evaluated. For both methods, the variable for the number of topics was tested from 5 to 20. Additionally for LDA, the alpha parameter was also tested from the choices of auto, symmetric, and asymmetric. Then the perplexity and topic coherence measures were analyzed to select several possible models. After investigating the top 25 tokens for each of those models to see which model made the most sense to a human, an NMF model with 6 topics was chosen as the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "#### Part A. Authorship Detection\n",
    "[Authorship Detection notebook](AuthorshipDetection.ipynb)\n",
    "##### <em>Mendenhall Curve Method</em>\n",
    "\n",
    "The analysis below is based on the work of literary scholar T.C. Mendenhall who theorized in 1887 that an author's stylistic signature could be determined by counting how often they used words of different lengths. While coarse by necessity of the time period in which he lived, the results of this analysis can be quickly generated and may provide some interesting insights (Fran√ßois Dominic Laram√©e, 2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Candidate Word Length Tendancies: Mendenhall's Characteristic Curve](Graphics/AD1.png \"Candidate Word Length Tendancies: Mendenhall's Characteristic Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying Mendenhall‚Äôs Characteristic Curves of Composition, there appears to be little difference in terms of word-length usage between Democratic Presidential Candidates on Twitter. Tom Steyer (the orange curve)--a businessman and non-career politician--had the most distinct curve of the group as he tended to use larger words more frequently than the other candidates. Could this be a sign of his naiveness about what constitutes a successul social media campaign for a politician? Additionally, two of the three woman candidates studied (Amy Klobuchar and Tulsi Gabbard) tended to use three-letter words much more frequently than their male-counterparts. This may signal a concerted effort by both campaigns to make their candidates appear more relatable by using simple and direct language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <em> Kilgariff's Chi-Squared Method <em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Kilgariff, in a 2001 paper, proposed using the chi-squared statistic to determine authorship. According to his method the 'statistic' measurement is a given author's 'distance' from the average use of the most frequent words in a comparison corpus--whether it is a collective corpus, an unknown writing sample, etc. Therefore, the author with a smallest 'statistic' tends to use the most commonly occuring words at a similar rate to the comparison corpus (Fran√ßois Dominic Laram√©e, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Determine the Number of Common Words Shared By All Candidates </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the number of tokens to use in this analysis is non-trivial. Some scholars suggest using between the 100 and 1,000 of the most common types in the corpus, while one researcher even recommended using every word that appeared at least twice. There appears to be some consensus, however, that the number of words selected should be proportional to the corpus size (larger corpus, larger number of common words used in the analysis) as to not give undue importance to infrequent words (Fran√ßois Dominic Laram√©e, 2018). In an attempt to be more scientific about selecting the critical number of types, a type-frequency diagram was constructed to identify an appropriate number of tokens to be used in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kilgariff Method: Selecting Number of Common Tokens](Graphics/AD2.png \"Kilgariff Method: Selecting Number of Common Tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this corpus is on the smaller size, the number of 'common tokens' should be proportionally small. As such, it is no surprise that this curve appears to level off around 100 types--this cut-off-value includes types such as '.', 'or', and 'their' but excludes slightly less common words such as 'iowa', 'community', 'want' and 'act.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Kilgariff Chi-Squared Method </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/AD3.png\" alt=\"Kilgariff Results\" width=\"200\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results calculated above, stylistically, we can say that Tom Steyer and Pete Buttigieg have writing styles most similar to the collective whole while Elizabeth Warren and Bernie Sanders have writing styles that are very different from the rest of Democratic Presidential Nominee Hopefuls. Note, just because two canidates have a score close to each other does not mean that their writing styles are similiar, rather that their writing styles are equi-distant from the collective group average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this technique revealed some clear differences between the candidates's tweeting styles, it may be possible to correctly identify  an author given a sufficient number of tweets. To test this hypothesis, the dataset was broken into training and testing datasets at a 80:20 ratio. Bootstrap samples of various tweet sizes (ranging from 1 to 100) were taken for each candidate from the testing dataset and passed into the Kilgariff Chi-Squared Algorithm. The candidate whose known tweets (from the training set) had a chi-squared statistic closest to zero would be assigned as the likely author. These results were then compared to the actual values and scored using the F1-Score.  The interesting results are below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kilgariff Prediction Score by Tweet Count](Graphics/AD4.png \"Kilgariff Prediction Score by Tweet Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Detailed Results for the \"Balanced Results\" model:\n",
    "\n",
    "<img src=\"Graphics/AD5.png\" alt=\"Balanced Results Details\" width=\"600\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Detailed Results for \"Best Performance\" model:\n",
    "\n",
    "<img src=\"Graphics/AD6.png\" alt=\"Best Performance Details\" width=\"600\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method shows a great deal of promise for authorship detection. Using just 75 tweets, an author could be detected from a field of seven candidates with greater than 98.6 percent accuracy. With just 50 tweets, the algorithm still runs well achieving nearly 80 percent accuracy. Future analyses should considering rerunning this analysis while also capturing if the correct candidate appeared in the \"Top X\" candidates.\n",
    "\n",
    "Specific to this analysis, the model tended to over-attribute tweets to Congresswoman Tulsi Gabbard--something observed in the two cases printed above but was also observed during the construction of the algorithm as well. It seems that her writing style closely matches that of a few other candidates, especially Senator Amy Klobuchar and Vice President Joe Biden.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <em> Simple Visualization of the Candidate's Tweets <em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/ANLY580_Project2_Authorship_FlagFinal.png\" alt=\"Tweet Flag\" width=\"600\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue field in the wordcloud represents the total corpus, that is tweets from all candidates. The lemmatized tokens displayed therein are unsuprising given the nature of political messaging--that is candidates want to *thank* their supporters for their support of their *presidential* campaigns, talk about their efforts to improve the *country,* and build support for the *need* of their agenda. Interestingly, Tulsi Gabbard's name appears prominently in the collective word cloud, perhaps because her tweets include her name frequently which might be an attempt to increase her name recognition. Also, President Trump's name appears prominently, which may be an indicator that the Democratic Primary was less about the party and its ideals, but more of an indictment of President Trump and his political agenda.\n",
    "\n",
    "The individual candidate wordclouds (respresented by the red stripes) reveal the key political messages of each candidate. Can you guess what stripe corresponds to which candidate? \n",
    "\n",
    "_\"Red Stripe\" Wordcloud Order (Top to Bottom): Mr. Tom Steyer, Rep. Tulsi Gabbard, Sen. Amy Klobuchar, Mayor Pete Buttigieg, Sen. Bernie Sanders, Sen. Elizabeth Warren, and V.P. Joe Biden._\n",
    "\n",
    "What is clear from this analysis is that candidates such as Senators Bernie Sanders and Elizabeth Warren are *fighting* for major *structural change* on behalf of the *people.* Curiously, these two individuals used the token *American* less frequently than most other candidates. It is clear that a theme of President-Elect Biden's campaign was \"Donald Trump\" himself, likely because his platform was largely centered on being the only candidate capable of defeating the incumbant president (a topic that is explored further in the topic modeling analysis). The messaging from other candidates isn't as clear from this analysis, but some show a propensity to use Twitter as a launch pad for other campaign events as evidence by the word, *live* and *now* clearly displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <em> Modern Machine Learning Approach</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based upon the singificant indicators of sylometric importance in the Castro-Lindauer paper, \"Authorship Identification on Twitter (2012),\" eight new features were generated from the tweets in this dataset (per tweet):\n",
    "- The Number of Characters\n",
    "- The Number of Tokens\n",
    "- The Frequency of Lowercase Word Use (i.e. candidate, debate)\n",
    "- The Frequency of Uppercase Word Use (i.e. ACTION, NOW, LIVE)\n",
    "- The Frequency of Titlecase Word Use (i.e. President, Support)\n",
    "- The Frequency of Non-ASCII Characters (i.e. emojis)\n",
    "- The Frequency of Mentions\n",
    "- The Frequency of Hashtags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to collinearity concerns, however, the number of characters per tweet and the titlecase word frequency metrics were removed in favor of the number of tokens per tweet and the lowercase word frequency features as they were shown to have a greater information gain during the Castro-Lindauer analysis. While a detailed readout of the summary statistics for these features can be seen in the Authorship Detection Notebook, a brief summary of the results is below.\n",
    "<br>\n",
    "Significant differences exist between candidates in terms of their Twitter stylometry, particularly Congresswoman Tulsi Gabbard. Compared to the other candidates in the primary, her Twitter stylometry was wide-ranging with inconsistent tweet lengths and varied use of mentions and hashtags. This could be a sign of poor message standards among her public affairs team or a sign that congresswoman herself primarily tweeted from this account with many \"off-the-cuff\" remarks. Ironically, Congresswoman Gabbard had very little non-ASCII character (emoji) usage despite her username having one. Only Senator Sanders had a lower proportion of tokens that were non-ASCII characters (just 8 in 1000 tokens were emojis or similar tokens). Only three candidates used hashtags with any frequency--Mayor Pete Buttigieg, Congresswoman Tulsi Gabbard, and Senator Elizabeth Warren--while only Congresswoman Gabbard used mentions (**@**) with any real consistency, which seems somewhat counter-intuitive. Interestingly, all three female candidates tended to use all uppercase tokens (i.e. \"NOT\", \"LIVE\", \"NOW\") significantly more than their male counterparts. Is this a sign of aggressive rhetoric by candidates trying to stir support or is a stylometric difference between men's and women's tweets writ large?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a firm understanding of the various stylometric measures, a machine learning pipeline was constructed in which these measures served as features to predict the tweet author (the label). Four models--Naive-Bayes, Ridge and Lasso Logistic Regression, Random Forest, and K-Nearest Neighbors--were constructed and tuned to optimize the hyperparameters for the single, best prediction. The top five predictions from each of the best-tuned models were extracted, compared to the actual labels, and scored using the accuracy measure. The results are below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/AuthorshipDetectionTrain.png\" alt=\"Tweet Flag\" width=\"600\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results seen above, the Random Forest and K-Nearest Neighbor models perfomed much better than the other models evaluated in this task. The Random Forest model was nearly 34 percent accurate at predicting the candidate (more than twice the accuracy of a random guess) and the correct tweet author was listed in the top 2 predictions 69 percent of the time. However, once the model was required to select the best three candidates the Random Forest and K-Nearest Neighbor models performed nearly the same and once the model had to essentially rule out the two candidates that the tweet was definitely not from, K-Nearest Neighbors was better. As a result, those two models were further evaluated using the test data to determine the best overall model. The results of this analysis are below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/AuthorshipDetectionTest.png\" alt=\"Tweet Flag\" width=\"600\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the Random Forest and K-Nearest Neighbor models had a measurable improvement for predicting the correct candidate as the first choice with the test data. This boost in prediction-ability is likely the result of an appropriately (and conservatively) trained model. This chart, however, clearly shows that the Random Forest model provides the best results correctly identifying the author of a tweet approximately 45 percent of the time as the first choice and more than 66 percent of the time as the second choice. <br>\n",
    "The model had the most success identifying tweets belonging to Congresswoman Gabbard, Senator Sanders, and Senator Warren--correctly identifying their tweets between 39 to 59 percent of the time. Conversely, the model had particular difficulty detecting tweets originating from the Mayor Buttigieg campaign, wherein only 8 percent of his tweets were correctly identified. The results are not unexpected--that is based on the results of the earlier Kilgariff Chi-Squared analysis, Mayor Buttigeig and Mr. Steyer (the second lowest F1 Score) had stylometrics very similiar to the group norm whereas Senators Sanders and Warren had tendancies very distinct styles. This stylistic similarity/dissimilarity to the group average could explain why the model succeeded or struggled to identify certain authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/AuthorshipDetectionEval.png\" alt=\"Tweet Flag\" width=\"600\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand how each feature contributed to the Random Forest  model's prediction, we improved upon the standard feature importance plot (which typically just shows one metric) to show the gini decrease, model entropy, and root count. Accross all three metrics, the frequency of using all *lowercase text in a tweet* was the most important feature in predicting an author, resulting in a Gini Decrease of just over .20 ¬± .087. The *number of tokens in the tweet* was the second important feature before a large gap in terms of the model's predictability. These findings align with those of the Castro-Lindauer paper, which found these two stylstic features are among the three most discriminating features. Interestingly, the frequency of using non-ASCII characters--a (distant) fourth most important feature in our model--was the most significant attribute in the Castro-Lindauer paper. This may be due to a more uniformly 'professional' writing style used by the seven Democratic Presidential Nominee Hopefuls studied in this paper, especially when compared to a more random sample of Twitter users (as was the case in the other paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graphics/AuthorshipDetectionFeat.png\" alt=\"Tweet Flag\" width=\"600\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B. Topic Modeling\n",
    "[Topic Modeling notebook](TopicModeling.ipynb)\n",
    "\n",
    "The selected NMF model had six topics, which were generalized as follows:\n",
    "1. Campaigning \n",
    "    - This topic is focused on early voting states like 'Iowa' and 'New Hampshire', asking people to 'join', and highlighting events like 'town halls'.\n",
    "2. Donations \n",
    "    - This topic is focused on fundraising with words like 'help', 'chip (in)', 'grassroots', 'donor', and 'donation'.\n",
    "3. Health Care and Climate Change \n",
    "    - This topic is mostly focused on 'health care', but 'climate change' is also present.\n",
    "4. Defeating Donald Trump \n",
    "    - This topic has some overlap with the third, since it has 'climate' and 'crisis', but tweets in this category are focused on how President Trump mishandled the crisis in their view.\n",
    "5. Workers' Rights and Education \n",
    "    - This topic is mostly focused on workers' rights with words like 'worker', 'union', and 'pay', but the less common words of 'student', 'school', and 'teacher' extends it to education.\n",
    "6. Gun Violence and Legislation \n",
    "    - This topics is mostly focused on gun violence with words like 'gun', 'violence', and 'epidemic', but the less common words of 'pass', 'law', 'house', and 'senate' extends it topic to general legislation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Wordclouds for Each Topic](Graphics/TopicModeling3.png \"Wordclouds for Each Topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Results for All Candidates</b>\n",
    "\n",
    "The Campaigning and Donations topics accounted for about 41% of the tweets, which shows that the candidates frequently used Twitter to advertise their events and fundraise. But the other 59% of tweets were more about policies they would enact or disagree with, which is somewhat surprising based on the short character length available in a tweet.\n",
    "\n",
    "The Gun Violence and Legislation topic had an unexpected number of tweets, which was caused by two factors. The first was the conversation driven by the mass shootings in El Paso, Dayton, and elsewhere in 2019. The second was the inclusion of many general political/legislative phrases in the topic, which would result in legislative tweets unrelated to gun violence also being categorized in this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Distribution of Tweets by Topic](Graphics/TopicModeling1.png \"Distribution of Tweets by Topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Results for Each Candidate </b>\n",
    "\n",
    "Here are some insights:\n",
    "- Warren was the closest candidate to tweeting about each topic the same amount and had proportionally more Donations tweets than the others.\n",
    "- Biden and Steyer had proportionally more Defeating Trump tweets.\n",
    "- Sanders had proportionally more Workers' Rights/Education tweets.\n",
    "- Gabbard and Klobuchar had more Campaigning tweets and less Health Care/Climate Change tweets than the rest of the candidates.\n",
    "- Buttigieg had proportionally more Gun Violence/Legislation tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "President-Elect Biden's message of the importance of defeating President Trump was not unique among the field of candidates, but he emphasized it the most and made it a key pillar of his campaign. This strategy appears to have worked for him, as he won both the Democratic nomination and the general election."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Percent of Candidates' Tweets Categorized as each Topic](Graphics/TopicModeling2.png \"Percent of Candidates' Tweets Categorized as each Topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This research shows that each of the seven Democratic Presidential Nominee Hopefuls tweeted differently (stylistically) to engage prospective voters about different issues related to their campaigns. Notably, President-Elect Joe Biden used Twitter to convince Democractic voters that he was the only candidate capable of beating the incumbent--President Donald Trump--a strategy that appears to have contributed to his successful campaign. Meanwhile, it is clear that lesser-known candidates (Congresswoman Gabbard, Senator Klobuchar, and Mayor Buttigieg) used the plaform to build support for their campaign through notification of campaign events and fundraising activities. Stylistically, the tweets of Senators Sanders and Warren diverged dramatically from the other candidates, a fact that made attributing unknown tweets to them occur with greater accuracy. Authorship detection proved much harder for Mayor Buttigieg--regardless of the technique used--likely indicating that his tweeting and linguistic styles are very similar to the several other candidates in the field. Regardless, it is clear that these candidates used Twitter differently from each other and for very different purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of authorship detection, there are two potential areas for future research. The first would be to compare the Kilgariff Chi-Squared Model and Random Forest Model head-to-head, specifically looking at which model is more effective given a single tweet, multiple tweets, and predicting the correct author in the Top 'X' predictions. The second area of future research would be to expand the number of features in the model to include the meta-data provided by Twitter (such as the tweet time and tweet location) and generated features (such as the tweet topic as generated by our NMF model). \n",
    "\n",
    "Additionally, this research was also going to evaluate the topic modeling techniques featured in J√≥nsson and Stolee's paper like biterm and word2vec topic models. Unfortunately, there was not enough time to complete these models and so this is another area for future research. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Castro, Antonio, and Brian Lindauer. Author Identification on Twitter. http://cs229.stanford.edu/proj2012/CastroLindauer-AuthorIdentificationOnTwitter.pdf. Accessed 22 Oct. 2020.\n",
    "\n",
    "Fran√ßois Dominic Laram√©e, \"Introduction to stylometry with Python,\" The Programming Historian 7 (2018), https://doi.org/10.46430/phen0078.\n",
    "\n",
    "J√≥nsson, Elƒ±as, and Jake Stolee. \"An evaluation of topic modelling techniques for twitter.\" (2015). http://www.cs.toronto.edu/~jstolee/projects/topic.pdf. Accessed 22 Oct. 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
